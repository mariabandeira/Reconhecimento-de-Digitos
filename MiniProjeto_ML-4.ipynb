{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import warnings\n",
    "import itertools\n",
    "import pickle\n",
    "from numpy import linalg as LA\n",
    "import random\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_redu_path = 'https://raw.githubusercontent.com/mariabandeira/Reconhecimento-de-Digitos/main/dataset_redu/test_redu.csv'\n",
    "train_redu_path = 'https://raw.githubusercontent.com/mariabandeira/Reconhecimento-de-Digitos/main/dataset_redu/train_redu.csv'\n",
    "\n",
    "train_redu = pd.read_csv(train_redu_path, sep=';')\n",
    "test_redu = pd.read_csv(test_redu_path, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Melhor sequência de dígitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalização para PLA e Regressão Logística\n",
    "scaler = RobustScaler()\n",
    "\n",
    "train_redu_scaled = scaler.fit_transform(train_redu.drop('label', axis=1)) # ajusta e transforma\n",
    "test_redu_scaled = scaler.transform(test_redu.drop('label', axis=1)) # apenas transforma (faz com que não haja vazamento de dados)\n",
    "\n",
    "train_redu['i_normalizada'] = train_redu_scaled[:,0]\n",
    "train_redu['s_normalizada'] = train_redu_scaled[:,1]\n",
    "\n",
    "test_redu['i_normalizada'] = test_redu_scaled[:,0]\n",
    "test_redu['s_normalizada'] = test_redu_scaled[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression_:\n",
    "  def __init__(self, eta=0.1, tmax=1000, bs=20):\n",
    "    self.eta = eta\n",
    "    self.tmax = tmax\n",
    "    self.batch_size = bs\n",
    "\n",
    "  # Infere o vetor w da funçao hipotese\n",
    "  #Executa a minimizao do erro de entropia cruzada pelo algoritmo gradiente de descida\n",
    "  def fit(self, _X, _y):\n",
    "    X = np.array(_X)\n",
    "    y = np.array(_y)\n",
    "\n",
    "    N = X.shape[0]\n",
    "    d = X.shape[1]\n",
    "    w = np.zeros(d, dtype=float)\n",
    "    self.w = []\n",
    "\n",
    "    for t in range(self.tmax):\n",
    "        vsoma = np.zeros(d, dtype=float)\n",
    "      \n",
    "        # Seleciona um lote de dados aleatorios\n",
    "        if self.batch_size < N:\n",
    "            idx = random.sample(range(N), self.batch_size)\n",
    "            batchX = [X[i] for i in idx]\n",
    "            batchY = [y[i] for i in idx]\n",
    "        else:\n",
    "            batchX = X\n",
    "            batchY = y\n",
    "\n",
    "        # Calcula o gradiente da funcao de erro\n",
    "        for xn, yn in zip(batchX, batchY):\n",
    "            vsoma += (yn * xn) / (1 + np.exp((yn * w).T @ xn))\n",
    "\n",
    "        grad_t = vsoma / len(batchY)\n",
    "        # condição de parada\n",
    "        if LA.norm(grad_t) < 0.0001:\n",
    "            break\n",
    "\n",
    "        w = w + (self.eta * grad_t)\n",
    "\n",
    "    self.w = w\n",
    "\n",
    "  #funcao hipotese inferida pela regressa logistica\n",
    "  def predict_prob(self, X):\n",
    "    s = np.dot(X, self.w)\n",
    "    prob = np.exp(s) / (1 + np.exp(s))\n",
    "    return prob\n",
    "\n",
    "  #Predicao por classificação linear\n",
    "  def predict(self, X):\n",
    "    prob = self.predict_prob(X)\n",
    "    y = np.where(prob >= 0.5, 1, -1)\n",
    "    return y\n",
    "\n",
    "  def getW(self):\n",
    "    return self.w\n",
    "\n",
    "  def getRegressionY(self, regressionX, shift=0):\n",
    "    return (-self.w[0]+shift - self.w[1]*regressionX) / self.w[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UmXtodos(train_redu, test_redu, digits=[0, 1, 4, 5]):\n",
    "    # Normalize train_redu\n",
    "    scaler = RobustScaler()\n",
    "    train_redu_scaled = scaler.fit_transform(train_redu.drop('label', axis=1))\n",
    "    train_redu['i_normalizada'] = train_redu_scaled[:, 0]\n",
    "    train_redu['s_normalizada'] = train_redu_scaled[:, 1]\n",
    "\n",
    "    test_redu_scaled = scaler.transform(test_redu.drop('label', axis=1))\n",
    "    test_redu['i_normalizada'] = test_redu_scaled[:, 0]\n",
    "    test_redu['s_normalizada'] = test_redu_scaled[:, 1]\n",
    "\n",
    "    modelosParent = []\n",
    "\n",
    "    def classificar_digito(modelos, x):\n",
    "        for modelo in modelos:\n",
    "            if modelo['modelo'].predict([x])[0] == 1:\n",
    "                return modelo['digito']\n",
    "        return 5\n",
    "\n",
    "    for d in digits:\n",
    "        modelosChild = {'digito': d}\n",
    "\n",
    "        X_train = np.hstack((np.ones((len(train_redu), 1)), train_redu[['i_normalizada', 's_normalizada']].values))\n",
    "        y_train = np.array([1 if y_ == d else -1 for y_ in train_redu['label']])\n",
    "\n",
    "        X_test = np.hstack((np.ones((len(test_redu), 1)), test_redu[['i_normalizada', 's_normalizada']].values))\n",
    "        y_test = np.array([1 if y_ == d else -1 for y_ in test_redu['label']])\n",
    "\n",
    "        modelo = LogisticRegression_(eta=0.1, tmax=1000, bs=20)\n",
    "        modelo.fit(X_train, y_train)\n",
    "\n",
    "        modelosChild['modelo'] = modelo\n",
    "        modelosParent.append(modelosChild)\n",
    "\n",
    "    for modelosChild in modelosParent:\n",
    "        modelo = modelosChild['modelo']\n",
    "        digit = modelosChild['digito']\n",
    "\n",
    "        train_redutemp = train_redu[train_redu['label'] == digit]\n",
    "        test_redutemp = test_redu[test_redu['label'] == digit]\n",
    "\n",
    "        X_train = np.hstack((np.ones((len(train_redutemp), 1)), train_redutemp[['i_normalizada', 's_normalizada']].values))\n",
    "        y_train = np.array([1 if y_ == digit else -1 for y_ in train_redutemp['label']])\n",
    "\n",
    "        X_test = np.hstack((np.ones((len(test_redutemp), 1)), test_redutemp[['i_normalizada', 's_normalizada']].values))\n",
    "        y_test = np.array([1 if y_ == digit else -1 for y_ in test_redutemp['label']])\n",
    "\n",
    "        # calcular ein e eout\n",
    "        eIn = sum(1 for i in range(len(X_train)) if classificar_digito(modelosParent, X_train[i]) != train_redutemp['label'].iloc[i])\n",
    "        eOut = sum(1 for i in range(len(X_test)) if classificar_digito(modelosParent, X_test[i]) != test_redutemp['label'].iloc[i])\n",
    "\n",
    "        modelosChild['eIn'] = eIn / len(X_train)\n",
    "        modelosChild['eOut'] = eOut / len(X_test)\n",
    "\n",
    "    return modelosParent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1, 4, 5),\n",
       " (0, 1, 5, 4),\n",
       " (0, 4, 1, 5),\n",
       " (0, 4, 5, 1),\n",
       " (0, 5, 1, 4),\n",
       " (0, 5, 4, 1),\n",
       " (1, 0, 4, 5),\n",
       " (1, 0, 5, 4),\n",
       " (1, 4, 0, 5),\n",
       " (1, 4, 5, 0),\n",
       " (1, 5, 0, 4),\n",
       " (1, 5, 4, 0),\n",
       " (4, 0, 1, 5),\n",
       " (4, 0, 5, 1),\n",
       " (4, 1, 0, 5),\n",
       " (4, 1, 5, 0),\n",
       " (4, 5, 0, 1),\n",
       " (4, 5, 1, 0),\n",
       " (5, 0, 1, 4),\n",
       " (5, 0, 4, 1),\n",
       " (5, 1, 0, 4),\n",
       " (5, 1, 4, 0),\n",
       " (5, 4, 0, 1),\n",
       " (5, 4, 1, 0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits = [0, 1, 4, 5]\n",
    "list(itertools.permutations(digits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Permutação 1/24\n",
      "Permutação: (0, 1, 4, 5)\n",
      "E_in médio: 0.2605882352941176\n",
      "E_out médio: 0.2580275229357798\n",
      "\n",
      "Permutação 2/24\n",
      "Permutação: (0, 1, 5, 4)\n",
      "E_in médio: 0.26294117647058823\n",
      "E_out médio: 0.2603211009174312\n",
      "\n",
      "Permutação 3/24\n",
      "Permutação: (0, 4, 1, 5)\n",
      "E_in médio: 0.2605882352941176\n",
      "E_out médio: 0.26146788990825687\n",
      "\n",
      "Permutação 4/24\n",
      "Permutação: (0, 4, 5, 1)\n",
      "E_in médio: 0.26588235294117646\n",
      "E_out médio: 0.26146788990825687\n",
      "\n",
      "Permutação 5/24\n",
      "Permutação: (0, 5, 1, 4)\n",
      "E_in médio: 0.2605882352941176\n",
      "E_out médio: 0.2580275229357798\n",
      "\n",
      "Permutação 6/24\n",
      "Permutação: (0, 5, 4, 1)\n",
      "E_in médio: 0.2605882352941176\n",
      "E_out médio: 0.26146788990825687\n",
      "\n",
      "Permutação 7/24\n",
      "Permutação: (1, 0, 4, 5)\n",
      "E_in médio: 0.26176470588235295\n",
      "E_out médio: 0.26146788990825687\n",
      "\n",
      "Permutação 8/24\n",
      "Permutação: (1, 0, 5, 4)\n",
      "E_in médio: 0.2605882352941176\n",
      "E_out médio: 0.2603211009174312\n",
      "\n",
      "Permutação 9/24\n",
      "Permutação: (1, 4, 0, 5)\n",
      "E_in médio: 0.2635294117647059\n",
      "E_out médio: 0.26146788990825687\n",
      "\n",
      "Permutação 10/24\n",
      "Permutação: (1, 4, 5, 0)\n",
      "E_in médio: 0.3205882352941176\n",
      "E_out médio: 0.3268348623853211\n",
      "\n",
      "Permutação 11/24\n",
      "Permutação: (1, 5, 0, 4)\n",
      "E_in médio: 0.3176470588235294\n",
      "E_out médio: 0.32798165137614677\n",
      "\n",
      "Permutação 12/24\n",
      "Permutação: (1, 5, 4, 0)\n",
      "E_in médio: 0.3170588235294118\n",
      "E_out médio: 0.3268348623853211\n",
      "\n",
      "Permutação 13/24\n",
      "Permutação: (4, 0, 1, 5)\n",
      "E_in médio: 0.2641176470588235\n",
      "E_out médio: 0.26146788990825687\n",
      "\n",
      "Permutação 14/24\n",
      "Permutação: (4, 0, 5, 1)\n",
      "E_in médio: 0.2605882352941176\n",
      "E_out médio: 0.2603211009174312\n",
      "\n",
      "Permutação 15/24\n",
      "Permutação: (4, 1, 0, 5)\n",
      "E_in médio: 0.2605882352941176\n",
      "E_out médio: 0.2591743119266055\n",
      "\n",
      "Permutação 16/24\n",
      "Permutação: (4, 1, 5, 0)\n",
      "E_in médio: 0.32588235294117646\n",
      "E_out médio: 0.3360091743119266\n",
      "\n",
      "Permutação 17/24\n",
      "Permutação: (4, 5, 0, 1)\n",
      "E_in médio: 0.32588235294117646\n",
      "E_out médio: 0.3348623853211009\n",
      "\n",
      "Permutação 18/24\n",
      "Permutação: (4, 5, 1, 0)\n",
      "E_in médio: 0.3411764705882353\n",
      "E_out médio: 0.34174311926605505\n",
      "\n",
      "Permutação 19/24\n",
      "Permutação: (5, 0, 1, 4)\n",
      "E_in médio: 0.33529411764705885\n",
      "E_out médio: 0.3394495412844037\n",
      "\n",
      "Permutação 20/24\n",
      "Permutação: (5, 0, 4, 1)\n",
      "E_in médio: 0.3311764705882353\n",
      "E_out médio: 0.3371559633027523\n",
      "\n",
      "Permutação 21/24\n",
      "Permutação: (5, 1, 0, 4)\n",
      "E_in médio: 0.3264705882352941\n",
      "E_out médio: 0.33256880733944955\n",
      "\n",
      "Permutação 22/24\n",
      "Permutação: (5, 1, 4, 0)\n",
      "E_in médio: 0.3235294117647059\n",
      "E_out médio: 0.33371559633027525\n",
      "\n",
      "Permutação 23/24\n",
      "Permutação: (5, 4, 0, 1)\n",
      "E_in médio: 0.33294117647058824\n",
      "E_out médio: 0.338302752293578\n",
      "\n",
      "Permutação 24/24\n",
      "Permutação: (5, 4, 1, 0)\n",
      "E_in médio: 0.3247058823529412\n",
      "E_out médio: 0.3348623853211009\n",
      "\n",
      "Melhor permutação: (0, 1, 4, 5)\n",
      "Melhor E_in mínimo: 0.2605882352941176\n",
      "Melhor E_out mínimo: 0.2580275229357798\n"
     ]
    }
   ],
   "source": [
    "# Lista de dígitos\n",
    "digits = [0, 1, 4, 5]\n",
    "\n",
    "# Gerar todas as permutações\n",
    "permutations = list(itertools.permutations(digits))\n",
    "\n",
    "# Inicializar variáveis para armazenar os melhores resultados\n",
    "best_permutation = None\n",
    "best_Ein_min = float('inf')\n",
    "best_Eout_min = float('inf')\n",
    "best_modelos = None\n",
    "\n",
    "# Testar todas as permutações\n",
    "for index, perm in enumerate(permutations):\n",
    "    print(f\"Permutação {index + 1}/{len(permutations)}\")\n",
    "    print(f\"Permutação: {perm}\")\n",
    "    modelos = UmXtodos(train_redu, test_redu, perm)\n",
    "    Eins = [m['eIn'] for m in modelos]\n",
    "    Eouts = [m['eOut'] for m in modelos]\n",
    "    \n",
    "    # medias\n",
    "    Ein_mean = np.mean(Eins)\n",
    "    Eout_mean = np.mean(Eouts)\n",
    "\n",
    "    # Verificar se é a melhor permutação\n",
    "    if Ein_mean < best_Ein_min:\n",
    "        best_Ein_min = Ein_mean\n",
    "        best_Eout_min = Eout_mean\n",
    "        best_permutation = perm\n",
    "        best_modelos = modelos\n",
    "\n",
    "    print(f\"E_in médio: {Ein_mean}\")\n",
    "    print(f\"E_out médio: {Eout_mean}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Exibir os melhores resultados\n",
    "print(f\"Melhor permutação: {best_permutation}\")\n",
    "print(f\"Melhor E_in mínimo: {best_Ein_min}\")\n",
    "print(f\"Melhor E_out mínimo: {best_Eout_min}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eIn: 0.2717291857273559\n",
      "eOut: 0.2878923766816143\n"
     ]
    }
   ],
   "source": [
    "def classificar_digito(modelos, x):\n",
    "        for modelo in modelos:\n",
    "            if modelo['modelo'].predict([x])[0] == 1:\n",
    "                return modelo['digito']\n",
    "        return 5\n",
    "\n",
    "X_train = np.hstack((np.ones((len(train_redu), 1)), train_redu[['i_normalizada', 's_normalizada']].values))\n",
    "y_train = np.array([1 if y_ == best_permutation[0] else -1 for y_ in train_redu['label']])\n",
    "\n",
    "X_test = np.hstack((np.ones((len(test_redu), 1)), test_redu[['i_normalizada', 's_normalizada']].values))\n",
    "y_test = np.array([1 if y_ == best_permutation[0] else -1 for y_ in test_redu['label']])\n",
    "modelo = LogisticRegression_(eta=0.1, tmax=1000, bs=20)\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# calcular ein e eout\n",
    "eIn = sum(1 for i in range(len(X_train)) if classificar_digito(best_modelos, X_train[i]) != train_redu['label'].iloc[i])\n",
    "eOut = sum(1 for i in range(len(X_test)) if classificar_digito(best_modelos, X_test[i]) != test_redu['label'].iloc[i])\n",
    "\n",
    "print(f\"eIn: {eIn / len(X_train)}\")\n",
    "print(f\"eOut: {eOut / len(X_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
